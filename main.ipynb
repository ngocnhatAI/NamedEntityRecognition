{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from prepro import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from Model.biGRU import *\n",
    "from Model.biLSTM import *\n",
    "from Model.uniGRU import *\n",
    "from Model.uniLSTM import *\n",
    "from Model.CNN_biGRU import * \n",
    "from Model.CNN_biLSTM import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc dữ liệu\n",
    "trainSentences = readfile(\"input/train.txt\")\n",
    "devSentences = readfile(\"input/dev.txt\")\n",
    "testSentences = readfile(\"input/test.txt\")\n",
    "\n",
    "# Thêm thông tin ký tự\n",
    "trainSentences = addCharInformation(trainSentences)\n",
    "devSentences = addCharInformation(devSentences)\n",
    "testSentences = addCharInformation(testSentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build wordEmbeddings, caseEmbeddings,.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIQUE LABELS VÀ WORDS\n",
    "uni_labels = set()\n",
    "uni_words = {}\n",
    "for dataset in [trainSentences, devSentences, testSentences]:\n",
    "    for sentence in dataset:\n",
    "        for token, char, label in sentence:\n",
    "            uni_labels.add(label)\n",
    "            uni_words[token.lower()] = True\n",
    "\n",
    "label2Idx = {tag: index for index, tag in enumerate(sorted(uni_labels))}\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "# case2Index và caseEmbeddings\n",
    "case2Idx = {\n",
    "    'numeric': 0, \n",
    "    'allLower': 1, \n",
    "    'allUpper': 2, \n",
    "    'initialUpper': 3, \n",
    "    'other': 4, \n",
    "    'mainly_numeric': 5, \n",
    "    'contains_digit': 6, \n",
    "    'PADDING_TOKEN':7\n",
    "}\n",
    "caseEmbeddings = np.identity(len(case2Idx), dtype='float32')\n",
    "\n",
    "# char2Index\n",
    "char2Idx = {\"PADDING\":0, \"UNKNOWN\":1}\n",
    "for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|\":\n",
    "    char2Idx[c] = len(char2Idx)\n",
    "\n",
    "# word2Index và wordEmbeddings\n",
    "word2Idx = {}\n",
    "wordEmbeddings = []\n",
    "fEmbeddings = open(\"/kaggle/input/ner-data/glove.6B.100d.txt\", encoding=\"utf-8\")\n",
    "for line in fEmbeddings:\n",
    "    split = line.strip().split(\" \")\n",
    "    word = split[0]\n",
    "    \n",
    "    if len(word2Idx) == 0:  # Add padding + unknown\n",
    "        word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.zeros(len(split)-1)  # Zero vector cho 'PADDING' word\n",
    "        wordEmbeddings.append(vector)\n",
    "        \n",
    "        word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "        vector = np.random.uniform(-0.25, 0.25, len(split)-1)\n",
    "        wordEmbeddings.append(vector)\n",
    "\n",
    "    if split[0].lower() in uni_words:\n",
    "        vector = np.array([float(num) for num in split[1:]])\n",
    "        wordEmbeddings.append(vector)\n",
    "        word2Idx[split[0]] = len(word2Idx)\n",
    "        \n",
    "fEmbeddings.close()\n",
    "wordEmbeddings = np.array(wordEmbeddings)\n",
    "\n",
    "# Tạo ma trận dữ liệu\n",
    "train_set = createMatrices(trainSentences, word2Idx, label2Idx, case2Idx, char2Idx)\n",
    "dev_set = createMatrices(devSentences, word2Idx, label2Idx, case2Idx, char2Idx)\n",
    "test_set = createMatrices(testSentences, word2Idx, label2Idx, case2Idx, char2Idx)\n",
    "\n",
    "# Tạo batch\n",
    "train_batch, train_batch_len = createBatches(train_set)\n",
    "dev_batch, dev_batch_len = createBatches(dev_set)\n",
    "test_batch, test_batch_len = createBatches(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    tokens, casing, chars, labels = zip(*batch)\n",
    "    \n",
    "    tokens = torch.stack([torch.tensor(t, dtype=torch.long) for t in tokens], dim=0)\n",
    "    casing = torch.stack([torch.tensor(c, dtype=torch.long) for c in casing], dim=0)\n",
    "    chars = torch.stack([torch.tensor(ch, dtype=torch.long) for ch in chars], dim=0)\n",
    "    labels = torch.stack([torch.tensor(l, dtype=torch.long) for l in labels], dim=0)\n",
    "    \n",
    "    # return tokens, labels\n",
    "    return tokens, casing, chars, labels\n",
    "\n",
    "\n",
    "def create_dataloader(batch, batch_len, batch_size=32, shuffle=True):\n",
    "    # Chia data thành các batch nhỏ\n",
    "    data = batch\n",
    "    data_len = batch_len\n",
    "    loaders = []\n",
    "    start = 0\n",
    "    for end in data_len:\n",
    "        batch_data = data[start:end]\n",
    "        start = end\n",
    "        if shuffle:\n",
    "            random.shuffle(batch_data)\n",
    "        # Tạo DataLoader cho từng batch\n",
    "        tokens, casing, chars, labels = zip(*batch_data)\n",
    "        dataset = list(zip(tokens, casing, chars, labels))\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "        loaders.append(loader)\n",
    "    return loaders\n",
    "\n",
    "\n",
    "batch_size = 32  \n",
    "train_loaders = create_dataloader(train_batch, train_batch_len, batch_size=batch_size, shuffle=True)\n",
    "dev_loaders = create_dataloader(dev_batch, dev_batch_len, batch_size=batch_size, shuffle=False)\n",
    "test_loaders = create_dataloader(test_batch, test_batch_len, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loaders, using_char):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for loader in data_loaders:\n",
    "            for batch in loader:\n",
    "                tokens, cases, chars, labels = batch\n",
    "                tokens = tokens.to(model.device)\n",
    "                cases  = cases.to(model.device)\n",
    "                chars  = chars.to(model.device)\n",
    "                labels = labels.to(model.device)\n",
    "                \n",
    "                if using_char: # IF CNN\n",
    "                    outputs = model(tokens, cases, chars)  # [batch_size, seq_len, num_labels]\n",
    "                else:\n",
    "                    outputs = model(tokens)                # [batch_size, seq_len, num_labels]\n",
    "                \n",
    "                preds = torch.argmax(outputs, dim=-1)  # [batch_size, seq_len]\n",
    "                all_preds.extend(preds.cpu().numpy().tolist())\n",
    "                all_labels.extend(labels.cpu().numpy().tolist())\n",
    "    \n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for pred_seq, true_seq in zip(all_preds, all_labels):\n",
    "        for p, t in zip(pred_seq, true_seq):\n",
    "                y_pred.append(p)\n",
    "                y_true.append(t)\n",
    "    \n",
    "    # Tính macro F1-score\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loaders, dev_loaders, optimizer, criterion,epochs, using_char):\n",
    "    for epoch in range(epochs):\n",
    "        batch_count = 0\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for loader in train_loaders:\n",
    "            for batch in loader:\n",
    "                tokens, cases, chars, labels = batch\n",
    "                tokens = tokens.to(model.device)\n",
    "                cases  = cases.to(model.device)\n",
    "                chars  = chars.to(model.device)\n",
    "                labels = labels.to(model.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if using_char:\n",
    "                    outputs = model(tokens, cases, chars)  # [batch_size, seq_len, num_labels]\n",
    "                else:\n",
    "                    outputs = model(tokens)  # [batch_size, seq_len, num_labels]\n",
    "                outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size*seq_len, num_labels]\n",
    "                labels = labels.view(-1)  # [batch_size*seq_len]\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "        \n",
    "        avg_loss = total_loss / batch_count\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Đánh giá trên tập train\n",
    "        train_prec, train_rec, train_f1 = evaluate_model(model, train_loaders, using_char)\n",
    "        print(f\"Train Precision: {train_prec:.4f}, Recall: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "        \n",
    "        # Đánh giá trên tập dev\n",
    "        dev_prec, dev_rec, dev_f1 = evaluate_model(model, dev_loaders, using_char)\n",
    "        print(f\"Dev Precision: {dev_prec:.4f}, Recall: {dev_rec:.4f}, F1: {dev_f1:.4f}\\n\")\n",
    "        \n",
    "    print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển mô hình sang device (GPU hoặc CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "vocab_size=wordEmbeddings.shape[0] \n",
    "word_emb_dim=wordEmbeddings.shape[1] \n",
    "word_embeddings=wordEmbeddings\n",
    "case_emb_dim=len(case2Idx)\n",
    "case_embeddings=caseEmbeddings \n",
    "char_size=len(char2Idx)\n",
    "char_emb_dim=30\n",
    "conv_out_channels=30\n",
    "conv_kernel_size=3\n",
    "lstm_hidden_size=200 \n",
    "num_labels=len(label2Idx)\n",
    "dropout=0.5 \n",
    "dropout_recurrent=0.25\n",
    "\n",
    "# Khởi tạo mô hình (sử dụng dữ liệu embedding đã cho)\n",
    "model = CNN_biLSTM(\n",
    "    vocab_size=vocab_size, \n",
    "    word_emb_dim=word_emb_dim, \n",
    "    word_embeddings=word_embeddings, \n",
    "    case_emb_dim=case_emb_dim,\n",
    "    case_embeddings=case_embeddings, \n",
    "    char_size=char_size, \n",
    "    char_emb_dim=char_emb_dim, \n",
    "    conv_out_channels=conv_out_channels, \n",
    "    conv_kernel_size=conv_kernel_size, \n",
    "    lstm_hidden_size=lstm_hidden_size, \n",
    "    num_labels=num_labels,\n",
    "    dropout=0.5, \n",
    "    dropout_recurrent=0.25,\n",
    "    device=device\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Khởi tạo Optimizer và Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss cho classification\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "train_model(model, train_loaders, dev_loaders, optimizer, criterion, epochs=10, using_char=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
